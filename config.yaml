# LLM Agent Configuration

# Ollama Server Settings
ollama:
  host: "localhost"
  port: 11434
  model: "qwen2.5-coder:7b"  # Default model (used when multi-model is disabled)

  timeout: 300  # General timeout for simple operations
  planning_timeout: 180  # Phase 1 Fix: Timeout for planning phase (3 min)
  execution_timeout: 300  # Phase 1 Fix: Timeout for execution phase (5 min)

  # Context window settings
  num_ctx: 8192
  num_predict: 6144  # Increased from 2048 for better multi-file tasks (Phase 1 Enhancement)
  temperature: 0.7
  top_p: 0.9

  # RAM preloading for 10-20x faster model swaps (Cursor optimization)
  keep_alive: "60m"  # Keep models in RAM for 1 hour

  # ========== MULTI-MODEL SYSTEM (CURSOR-STYLE ARCHITECTURE) ==========
  multi_model:
    enabled: true  # Enable intelligent model routing

    # Architecture: openthinker3-7b orchestrates, delegates to specialists
    # Optimized for RTX 2070 8GB VRAM
    models:
      # Main Orchestrator - Central "brain" for all tasks
      orchestrator:
        name: "openthinker3-7b"
        description: "Main orchestrator - coordinates all tasks"
        use_for: ["coordination", "reasoning", "planning", "orchestration", "task_analysis"]
        vram_usage: "4.7GB"
        notes: "Primary model loaded for all tasks. Decides when to delegate to specialists."

      # Tool Call Formatter - Reliable structured output generation
      tool_formatter:
        name: "phi3:mini"
        description: "Reliable tool call formatting"
        use_for: ["tool_call_generation", "structured_output", "tool_formatting"]
        vram_usage: "2.2GB"
        notes: "Fixes 'qwen isnt good for tool calls'. Small enough to keep loaded with orchestrator (6.9GB total)."

      # Code Generation Specialist - Fast, specialized code writing
      code_generation:
        name: "qwen2.5-coder:7b"
        description: "Code generation specialist"
        use_for: ["code_generation", "code_refactoring", "multi_file_code"]
        vram_usage: "4.7GB"
        notes: "Delegated to by orchestrator for code writing tasks. Swapped in from RAM as needed (~500ms)."

      # Advanced Reasoning - For complex debugging and analysis
      advanced_reasoning:
        name: "llama3.1:8b"
        description: "Advanced reasoning for complex tasks"
        use_for: ["debugging", "complex_analysis", "edge_case_handling"]
        enabled: true  # Can be used when orchestrator needs extra reasoning power
        vram_usage: "4.9GB"
        notes: "Larger model for complex debugging. Use sparingly (model swap overhead)."

      # Heavy Reasoning - Very complex tasks only (disabled by default)
      complex_reasoning:
        name: "deepseek-r1:14b"
        description: "Heavy reasoning for extremely complex tasks"
        use_for: ["complex_reasoning", "multi_step_analysis"]
        enabled: false  # 9GB VRAM - maxes out GPU alone, slow
        vram_usage: "9.0GB"
        notes: "Disabled by default. Exceeds VRAM alone, requires CPU/RAM offloading (very slow)."

    # Routing configuration
    routing:
      auto_select: true  # Automatically pick best model for each task

      # Cursor-style routing strategy
      style: "cursor"  # Options: "cursor" (new), "hybrid" (legacy)

      cursor_routing:
        # Simple path: Direct orchestration (65-70% of tasks)
        simple_path:
          target_percentage: 0.65  # Aim for 65% of tasks
          orchestrator_model: "openthinker3-7b"
          tool_formatter_model: "phi3:mini"
          code_delegate_model: "qwen2.5-coder:7b"

          triggers:
            max_files: 2  # Use simple path if <= 2 files
            max_complexity: "standard"  # simple or standard complexity
            not_creative: true  # Avoid simple path if creative

        # Complex path: Planning → Validation → Orchestrated execution (30-35% of tasks)
        complex_path:
          target_percentage: 0.35  # Aim for 35% of tasks
          planning_model: "openthinker3-7b"
          orchestrator_model: "openthinker3-7b"
          code_delegate_model: "qwen2.5-coder:7b"
          tool_formatter_model: "phi3:mini"

          triggers:
            min_files: 3  # Use complex path if >= 3 files
            min_complexity: "complex"  # complex tier
            is_creative: true  # Use complex path if creative

        # Delegation rules
        delegation:
          code_generation_threshold: 20  # Delegate to qwen if >20 lines of code
          use_advanced_reasoning: false  # Use llama3.1:8b for debugging (disabled by default)
          tool_calls_always_use_formatter: true  # Always use phi3:mini for tool calls

      # Two-phase execution settings (now "complex path" in Cursor-style)
      two_phase:
        enabled: true  # Enable plan → execute workflow
        use_for_complex_creative: true  # Use for complex creative tasks
        planning_model: "openthinker3-7b"  # Model for Phase 1 (planning)
        execution_model: "qwen2.5-coder:7b"  # Model for Phase 2 (execution)
        orchestrator_model: "openthinker3-7b"  # NEW: Orchestrator coordinates execution

        # Plan validation settings (Phase 3)
        validation:
          enabled: true  # Enable plan validation and refinement
          max_refinement_iterations: 2  # Maximum plan refinement attempts
          min_plan_score: 0.7  # Minimum score for plan to be considered valid
          full_plan_in_execution: true  # Pass complete plan, don't truncate

        # Execution settings (Phase 3)
        execution:
          max_tokens: 6144  # Maximum tokens for execution (increased from 4096)
          early_termination_on_critical_failure: true  # Stop execution on critical errors
          monitor_execution: true  # Monitor tool execution results

        # Feedback loop settings (Phase 3)
        feedback_loop:
          enabled: true  # Enable execution feedback loop
          replan_on_failure_rate: 0.5  # Replan if success rate < 50%
          max_replan_attempts: 1  # Maximum replanning attempts

    # Streaming execution (Phase 1 Enhancement)
    streaming:
      enabled: true  # Enable real-time progress feedback
      use_rich_progress: true  # Use Rich library for fancy progress bars
      show_thinking: true  # Show LLM reasoning in real-time
      show_tool_execution: true  # Show tool calls as they execute

# Agent Settings
agent:
  name: "Pi Agent"
  workspace: "c:\\Users\\jluca\\Documents\\newfolder\\agent_workspace"
  max_iterations: 10
  
  # Memory settings
  enable_memory: true
  memory_file: "logs/agent_memory.json"
  max_memory_entries: 1000
  
  # Session history
  enable_session_history: true
  session_history_file: "logs/session_history.json"
  max_history_messages: 50
  
  # Context management
  summarize_long_conversations: true
  summary_threshold: 20  # Summarize after this many messages
  
# Security Settings
security:
  # Allowed directories for file operations (relative to workspace)
  allowed_paths:
    - "."
    - "projects"
    - "documents"
    - "temp"
  
  # Whitelisted commands (only these commands can be executed)
  allowed_commands:
    - "ls"
    - "pwd"
    - "whoami"
    - "date"
    - "echo"
    - "cat"
    - "grep"
    - "find"
    - "df"
    - "free"
    - "uptime"
    - "ps"
    - "pip"
    - "python"
    - "flake8"
    - "pytest"
    - "unittest"
  
  # Maximum file size for operations (in bytes)
  max_file_size: 10485760  # 10MB
  
  # Enable/disable command execution
  allow_command_execution: true
  
  # Enable/disable file operations
  allow_file_operations: true

  # Rate limiting (requests per minute)
  rate_limits:
    default_per_minute: 60  # Default limit for unlisted tools
    write_file_per_minute: 30
    execute_command_per_minute: 10
    http_request_per_minute: 20

  # Resource quotas
  resource_quotas:
    max_cpu_percent: 90  # Maximum CPU usage
    max_memory_mb: 2048  # Maximum memory usage (2GB)
    max_disk_mb: 10240  # Minimum free disk space required (10GB)

# Linter Settings (Phase 2)
linter:
  enabled: ["flake8"]  # Available: flake8, pylint
  max_issues: 10  # Maximum issues to report
  ignore:  # Error codes to ignore
    - "E501"  # Line too long (let code be readable)
    - "W503"  # Line break before binary operator
  severity_threshold: "warning"  # Minimum severity to report
  auto_fix: false  # Future: automatically fix simple issues

# Performance & Caching Settings (Phase 4)
performance:
  cache:
    system_info_ttl: 30  # Cache system info for 30 seconds
    file_search_ttl: 60  # Cache file search results for 60 seconds
    default_ttl: 300     # Default cache TTL (5 minutes)

  network:
    max_retries: 3         # HTTP retry attempts
    backoff_factor: 0.3    # Wait time between retries (0.3, 0.6, 1.2 seconds)
    pool_connections: 10   # HTTP connection pool size per host
    pool_maxsize: 10       # Maximum pool size
    timeout: 10            # Request timeout in seconds

# Execution History Settings (Phase 2)
execution_history:
  enabled: true
  database_path: "logs/execution_history.db"
  retention_days: 90  # Keep execution history for 90 days
  track_tool_details: true  # Track individual tool execution results

# Error Recovery Settings (Phase 2)
error_recovery:
  enabled: true
  max_retries: 3  # Maximum recovery attempts per error
  log_recovery_attempts: true  # Log all recovery attempts to history

# Semantic Context Settings (Phase 3)
context:
  semantic:
    enabled: false  # Enable semantic context engine (requires RAG setup)

    embedding:
      model: "sentence-transformers/all-MiniLM-L6-v2"  # Embedding model
      cache_embeddings: true  # Cache embeddings for faster searches
      update_interval: 300  # Re-embed changed files every 5 minutes

    relevance:
      min_similarity: 0.3  # Minimum similarity score (0-1) for file inclusion
      max_files: 10  # Maximum number of files to include in context
      chunk_size: 500  # Tokens per file chunk
      include_dependencies: true  # Include files from dependency graph

    prioritization:
      token_budget: 6000  # Token budget for context (reserve 2K for prompt/response)
      weights:
        critical: 1.0  # Files explicitly mentioned in query
        high: 0.7      # High semantic similarity
        medium: 0.4    # Medium similarity or dependencies
        low: 0.2       # General project context

  dependency_graph:
    enabled: false  # Enable dependency graph builder
    cache_path: "logs/dependency_graph.pkl"  # Cache file path
    rebuild_on_file_change: true  # Rebuild when files change
    max_depth: 2  # Maximum hops in dependency traversal

# Metrics & Observability Settings (Phase 6)
metrics:
  enabled: true
  slow_threshold_ms: 1000  # Operations slower than this are flagged
  export_on_shutdown: true
  output_dir: "logs"       # Directory for metrics exports

# Logging Settings
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  log_file: "logs/agent.log"
  max_log_size: 10485760  # 10MB
  backup_count: 5